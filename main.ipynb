{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffb8edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.\n",
    "        builder.\n",
    "        appName(\"curso_pyspark\").\n",
    "        getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15364912",
   "metadata": {},
   "source": [
    "# Básico de Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32c03ee",
   "metadata": {},
   "source": [
    "## Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb6e648",
   "metadata": {},
   "source": [
    "<!-- ###\n",
    "### create table (\n",
    "###    nome VARCHAR(100) not null,\n",
    "###    sobrenome VARCHAR(100) not null,\n",
    "###    idade INT not null\n",
    "###)\n",
    "\n",
    "#  nome sobrenome idade\n",
    "# (N1      N2       I1) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a09180b",
   "metadata": {},
   "source": [
    "create table ( <br />\n",
    "   nome VARCHAR(100) not null, <br />\n",
    "   sobrenome VARCHAR(100) not null, <br />\n",
    "   idade INT not null <br />\n",
    ") <br />\n",
    "<br /><br />\n",
    "nome sobrenome idade\n",
    "(N1      N2       I1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9041fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n",
    "\n",
    "data = [\n",
    "    ##  C1          C2        C3\n",
    "    (\"Matheus\", \"Cantarutti\", 31),\n",
    "    (\"Ana\", \"Cláudia\", 18),\n",
    "    (\"Brunno\", \"Oliveira\", 25)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Nome\", StringType(), True),\n",
    "    StructField(\"Sobre_Nome\", StringType(), True),\n",
    "    StructField(\"Idade\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.printSchema()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f854be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"pessoas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b349c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SQL --> Pyspark\n",
    "\n",
    "spark.sql( \n",
    "'''    \n",
    "    select\n",
    "        *\n",
    "    from pessoas\n",
    "    where Idade < 20\n",
    "'''\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe949c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# df.filter('Idade < 20').show()\n",
    "df.filter(\n",
    "    F.col('Idade') < 20\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd31a82",
   "metadata": {},
   "source": [
    "## Tipo de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6e024d",
   "metadata": {},
   "source": [
    "- TIPO TEXTO/STRING >> abrangendo apenas as funções que tratam texto\n",
    "- TIPO DATA (DATA ESTÁ COM O TIPO DE STRING) >> Converter o seu texto para Data\n",
    "\n",
    "- FLOAT/DECIMAL e INTERGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "    select\n",
    "        *,\n",
    "        cast(Idade * 5 as string) as Idade_2\n",
    "    from pessoas\n",
    "''').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2451fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(   \n",
    "    # nome da coluna, expressões/funcao\n",
    "    df.withColumn('Idade_2', F.col('Idade') * 5)\n",
    "      .withColumn('data', F.lit('2025-01-01')) # current date\n",
    "      .withColumn('data2', F.to_date(F.col('data'), 'yyyy-MM-dd'))\n",
    "      .withColumn('Idade_3', F.expr('cast(Idade * 5 as string) as Idade_3'))\n",
    "\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfc1d7d",
   "metadata": {},
   "source": [
    "## Cardinalidade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4765949",
   "metadata": {},
   "source": [
    "- Aula teórica explicativa sobre cardinalidade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e56f099",
   "metadata": {},
   "source": [
    "## Dataframe de exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f55a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType\n",
    "\n",
    "spark = (\n",
    "    SparkSession.\n",
    "        builder.\n",
    "        getOrCreate()\n",
    ")\n",
    "# CLOUD --> spark.sql('select * from bd.aulas.tabelas')\n",
    "# df <- na leitura dos arquivos (.csv ou xlsx)\n",
    "## spark.read\n",
    "\n",
    "data = [\n",
    "    (\"1\", \"PAGO\"),\n",
    "    (\"2\", \"APROVADO\"),\n",
    "    (\"3\", \"RECUSADO\"),\n",
    "    (\"4\", \"ENTREGUE\"),\n",
    "    (\"5\", \"CANCELADO\"),\n",
    "    (\"6\", \"NÃO ENTREGUE\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"cd_identificacao\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac45cec",
   "metadata": {},
   "source": [
    "# Lendo arquivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907ba94c",
   "metadata": {},
   "source": [
    "## Command Separated Value (.csv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c81002",
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho = './dados/csv/'\n",
    "cliente = 'clientes.csv'\n",
    "status = 'status.csv'\n",
    "pedidos = 'pedidos.csv'\n",
    "\n",
    "clientes = (\n",
    "    spark.read.csv(\n",
    "        f'{caminho}{cliente}', \n",
    "        sep=';',\n",
    "        header=True\n",
    "    )\n",
    ")\n",
    "\n",
    "status = (\n",
    "    spark.read.csv(\n",
    "        f'{caminho}{status}', \n",
    "        sep=';',\n",
    "        header=True\n",
    "    )\n",
    ")\n",
    "\n",
    "pedidos = (\n",
    "    spark.read.csv(\n",
    "        f'{caminho}{pedidos}', \n",
    "        sep=';',\n",
    "        header=True\n",
    "    )\n",
    ")\n",
    "\n",
    "pedidos.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376f2ad7",
   "metadata": {},
   "source": [
    "## Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2aad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "caminho = './dados/xlsx/'\n",
    "cliente = 'clientes.xlsx'\n",
    "aba = 'clientes'\n",
    "\n",
    "def ler_excel(file_path, aba):\n",
    "    try:\n",
    "        df = pd.read_excel(file_path, sheet_name=aba, engine='openpyxl')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Arquivo {file_path} não encontrado.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Erro ao ler a aba de nome {aba}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro inesperado: {e}\")\n",
    "    return df\n",
    "\n",
    "df = ler_excel(f'{caminho}{cliente}', aba)\n",
    "df = spark.createDataFrame(df)\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f649eb2a",
   "metadata": {},
   "source": [
    "# Intermediário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c28d6df",
   "metadata": {},
   "source": [
    "## Filtros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ae534",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d456be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5da915",
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho = './dados/csv/'\n",
    "cliente = 'clientes.csv'\n",
    "\n",
    "clientes = (\n",
    "    spark.read.csv(\n",
    "        f'{caminho}{cliente}', \n",
    "        sep=';',\n",
    "        header=True\n",
    "    )\n",
    ")\n",
    "\n",
    "clientes.show()\n",
    "clientes.createOrReplaceTempView(\"clientes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67133f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "    select \n",
    "        * \n",
    "    from clientes\n",
    "    where sexo = 'F'\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e003331",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes2 = (\n",
    "    # para filtrar dados, o filter é a função utilizada\n",
    "    clientes\n",
    "        .filter(\n",
    "            F.col('sexo') == 'F'\n",
    "        )\n",
    ")\n",
    "clientes2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a548efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes3 = (\n",
    "    # para filtrar dados, o filter é a função utilizada\n",
    "    clientes\n",
    "        .filter(\n",
    "            F.col('sexo') == 'M'\n",
    "        )\n",
    ")\n",
    "clientes3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0673f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes4 = (\n",
    "    # para filtrar dados, o filter é a função utilizada\n",
    "    clientes\n",
    "        .filter(\n",
    "            ~ (F.col('sexo') == 'F')\n",
    "        )\n",
    ")\n",
    "clientes4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce813ed1",
   "metadata": {},
   "source": [
    "### Isin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef2c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "    select\n",
    "        *\n",
    "    from clientes\n",
    "    where cd_cliente in ('2', '3')\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474f68d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "    select\n",
    "        *\n",
    "    from clientes\n",
    "    where cd_cliente not in ('2', '3')\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9bfae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes.filter(\n",
    "    F.col('cd_cliente').isin(['2', '3'])\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029334e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes.filter(\n",
    "    ~(F.col('cd_cliente').isin(['2', '3']))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5869cc71",
   "metadata": {},
   "source": [
    "## Tratamentos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d834083d",
   "metadata": {},
   "source": [
    "### Strings para Números"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b737e3",
   "metadata": {},
   "source": [
    "### regex com Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho = './dados/csv/'\n",
    "pedidos = 'pedidos.csv'\n",
    "\n",
    "pedidos = (\n",
    "    spark.read.csv(\n",
    "        f'{caminho}{pedidos}', \n",
    "        sep=';',\n",
    "        header=True\n",
    "    )\n",
    ")\n",
    "\n",
    "pedidos.show()\n",
    "pedidos.createOrReplaceTempView(\"pedidos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bd186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pedidos.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93904e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType, DoubleType\n",
    "\n",
    "pedidos2 = (\n",
    "    # MacOS é diferente do Windows\n",
    "    pedidos.withColumn('valor_limpo', F.regexp_replace(F.col('valor'),  r'R\\$\\s*', ''))\n",
    "           .withColumn('valor_limpo', F.regexp_replace(F.col('valor_limpo'), r'\\.', ''))\n",
    "           .withColumn('valor_limpo', F.regexp_replace(F.col('valor_limpo'), r',', '.'))\n",
    "           .withColumn('valor_limpo', F.col('valor_limpo').cast(DoubleType()))\n",
    ")\n",
    "\n",
    "pedidos2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce78d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pedidos2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a46368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pedidos2.groupBy('cd_cliente').agg(\n",
    "    F.mean('valor_limpo').alias('valor_mean_limpo'),\n",
    "    F.sum('valor_limpo').alias('valor_sum_limpo'),\n",
    "    F.max('valor_limpo').alias('valor_max_limpo'),\n",
    "    F.min('valor_limpo').alias('valor_min_limpo'),\n",
    "    F.avg('valor_limpo').alias('valor_avg_limpo'),\n",
    "    F.median('valor_limpo').alias('valor_median_limpo'),\n",
    "    F.mode('valor_limpo').alias('valor_mode_limpo')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ab098e",
   "metadata": {},
   "source": [
    "### Trabalhando com Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e74e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.\n",
    "        builder.\n",
    "        appName(\"curso_pyspark\").\n",
    "        getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f8059",
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho = './dados/csv/'\n",
    "clientes = 'clientes.csv'\n",
    "\n",
    "clientes = (\n",
    "    spark.read.csv(\n",
    "        f'{caminho}{clientes}', \n",
    "        sep=';',\n",
    "        header=True\n",
    "    )\n",
    ")\n",
    "\n",
    "clientes.show()\n",
    "clientes.createOrReplaceTempView(\"clientes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7887ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes3 = (\n",
    "    clientes\n",
    "        .select(\"cd_cliente\", \"data_nascimento\")\n",
    "        .withColumn(\"data_nascimento2\", F.to_date(\n",
    "            F.col(\"data_nascimento\"), \"yyyy-MM-dd\"\n",
    "        ))\n",
    "        .withColumn(\"ano\", F.year(F.col(\"data_nascimento2\")))\n",
    "        .withColumn(\"mes\", F.month(F.col(\"data_nascimento2\")))\n",
    "        .withColumn(\"dia\", F.day(F.col(\"data_nascimento2\")))\n",
    "        \n",
    "        .withColumn(\n",
    "            \"data_BR\", \n",
    "                F.concat(\n",
    "                    F.col(\"dia\"), # dia\n",
    "                        F.lit(\"/\"), \n",
    "                    F.col(\"mes\"), # mes\n",
    "                        F.lit(\"/\"), \n",
    "                    F.col(\"ano\") # ano\n",
    "                )\n",
    "        )\n",
    "        .withColumn(\"data_BR\", F.try_to_timestamp(\n",
    "            F.col(\"data_BR\"), \"dd/MM/yyyy\"\n",
    "        ))\n",
    "\n",
    ")\n",
    "\n",
    "clientes3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe27dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pode substituir a forma pela qual a conversão de data ocorre.\n",
    "        .withColumn(\n",
    "            \"data_BR_str\",\n",
    "            F.concat(\n",
    "                F.lpad(F.col(\"dia\"), 2, \"0\"), F.lit(\"/\"),\n",
    "                F.lpad(F.col(\"mes\"), 2, \"0\"), F.lit(\"/\"),\n",
    "                F.col(\"ano\")\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\"data_BR\", F.to_date(F.col(\"data_BR_str\"), \"dd/MM/yyyy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3ce058",
   "metadata": {},
   "source": [
    "## Um pouco mais de Modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4dadef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType\n",
    "\n",
    "caminho = './dados/csv/'\n",
    "pedidos = 'pedidos.csv'\n",
    "clientes = 'clientes.csv'\n",
    "status = 'status.csv'\n",
    "\n",
    "pedidos = (\n",
    "    spark.read.csv(\n",
    "        f'{caminho}{pedidos}', \n",
    "        sep=';',\n",
    "        header=True\n",
    "    )\n",
    ")\n",
    "\n",
    "clientes = (\n",
    "    spark.read.csv(\n",
    "        f'{caminho}{clientes}', \n",
    "        sep=';',\n",
    "        header=True\n",
    "    )\n",
    ")\n",
    "\n",
    "status = (\n",
    "    spark.read.csv(\n",
    "        f'{caminho}{status}', \n",
    "        sep=';',\n",
    "        header=True\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a333e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pedidos.groupBy('cd_cliente').agg(\n",
    "    F.count('cd_cliente').alias('count')\n",
    ").orderBy(\n",
    "    F.col('count').desc()\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef9e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "pedidos.groupBy('cd_cliente').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49344e2c",
   "metadata": {},
   "source": [
    "# Avançado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac8dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType\n",
    "\n",
    "spark = (\n",
    "    SparkSession.\n",
    "        builder.\n",
    "        appName(\"curso_pyspark\").\n",
    "        getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2303c312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ler_arquivo(caminho, nome_arquivo):\n",
    "    arquivo = (\n",
    "        spark.read.csv(\n",
    "            f'{caminho}{nome_arquivo}',\n",
    "            sep=';',\n",
    "            header=True\n",
    "        )\n",
    "    )\n",
    "    return arquivo\n",
    "\n",
    "pedidos = ler_arquivo('./dados/csv/', 'pedidos.csv')\n",
    "\n",
    "pedidos.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08640b13",
   "metadata": {},
   "source": [
    "## Join - Movimentando informações entre tabelas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb6e69c",
   "metadata": {},
   "source": [
    "### Busca Horizontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c45a05d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b005534f",
   "metadata": {},
   "source": [
    "### 'Busca' vertical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3a11ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eaa960ee",
   "metadata": {},
   "source": [
    "## Condicionais com When"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab07595c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de8e46cb",
   "metadata": {},
   "source": [
    "## Pivot tables\n",
    "### Transformando visões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01461b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
