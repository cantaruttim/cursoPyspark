{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ffb8edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/07 22:38:16 WARN Utils: Your hostname, Matheuss-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.135 instead (on interface en0)\n",
      "25/08/07 22:38:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/07 22:38:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.135:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>curso_pyspark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x120921d30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.\n",
    "        builder.\n",
    "        appName(\"curso_pyspark\").\n",
    "        getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15364912",
   "metadata": {},
   "source": [
    "# Básico de Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32c03ee",
   "metadata": {},
   "source": [
    "## Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb6e648",
   "metadata": {},
   "source": [
    "<!-- ###\n",
    "### create table (\n",
    "###    nome VARCHAR(100) not null,\n",
    "###    sobrenome VARCHAR(100) not null,\n",
    "###    idade INT not null\n",
    "###)\n",
    "\n",
    "#  nome sobrenome idade\n",
    "# (N1      N2       I1) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a09180b",
   "metadata": {},
   "source": [
    "create table ( <br />\n",
    "   nome VARCHAR(100) not null, <br />\n",
    "   sobrenome VARCHAR(100) not null, <br />\n",
    "   idade INT not null <br />\n",
    ") <br />\n",
    "<br /><br />\n",
    "nome sobrenome idade\n",
    "(N1      N2       I1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9041fe05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Nome: string (nullable = true)\n",
      " |-- Sobre_Nome: string (nullable = true)\n",
      " |-- Idade: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+\n",
      "|   Nome|Sobre_Nome|Idade|\n",
      "+-------+----------+-----+\n",
      "|Matheus|Cantarutti|   31|\n",
      "|    Ana|   Cláudia|   18|\n",
      "| Brunno|  Oliveira|   25|\n",
      "+-------+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n",
    "\n",
    "data = [\n",
    "    ##  C1          C2        C3\n",
    "    (\"Matheus\", \"Cantarutti\", 31),\n",
    "    (\"Ana\", \"Cláudia\", 18),\n",
    "    (\"Brunno\", \"Oliveira\", 25)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Nome\", StringType(), True),\n",
    "    StructField(\"Sobre_Nome\", StringType(), True),\n",
    "    StructField(\"Idade\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.printSchema()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72f854be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"pessoas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b349c65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----+\n",
      "|Nome|Sobre_Nome|Idade|\n",
      "+----+----------+-----+\n",
      "| Ana|   Cláudia|   18|\n",
      "+----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### SQL --> Pyspark\n",
    "\n",
    "spark.sql( \n",
    "'''    \n",
    "    select\n",
    "        *\n",
    "    from pessoas\n",
    "    where Idade < 20\n",
    "'''\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fe949c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----+\n",
      "|Nome|Sobre_Nome|Idade|\n",
      "+----+----------+-----+\n",
      "| Ana|   Cláudia|   18|\n",
      "+----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# df.filter('Idade < 20').show()\n",
    "df.filter(\n",
    "    F.col('Idade') < 20\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd31a82",
   "metadata": {},
   "source": [
    "## Tipo de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6e024d",
   "metadata": {},
   "source": [
    "- TIPO TEXTO/STRING >> abrangendo apenas as funções que tratam texto\n",
    "- TIPO DATA (DATA ESTÁ COM O TIPO DE STRING) >> Converter o seu texto para Data\n",
    "\n",
    "- FLOAT/DECIMAL e INTERGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6bc72e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Nome: string (nullable = true)\n",
      " |-- Sobre_Nome: string (nullable = true)\n",
      " |-- Idade: integer (nullable = true)\n",
      " |-- Idade_2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    select\n",
    "        *,\n",
    "        cast(Idade * 5 as string) as Idade_2\n",
    "    from pessoas\n",
    "''').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2451fe4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+-------+----------+----------+-------+\n",
      "|   Nome|Sobre_Nome|Idade|Idade_2|      data|     data2|Idade_3|\n",
      "+-------+----------+-----+-------+----------+----------+-------+\n",
      "|Matheus|Cantarutti|   31|    155|2025-01-01|2025-01-01|    155|\n",
      "|    Ana|   Cláudia|   18|     90|2025-01-01|2025-01-01|     90|\n",
      "| Brunno|  Oliveira|   25|    125|2025-01-01|2025-01-01|    125|\n",
      "+-------+----------+-----+-------+----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(   \n",
    "    # nome da coluna, expressões/funcao\n",
    "    df.withColumn('Idade_2', F.col('Idade') * 5)\n",
    "      .withColumn('data', F.lit('2025-01-01')) # current date\n",
    "      .withColumn('data2', F.to_date(F.col('data'), 'yyyy-MM-dd'))\n",
    "      .withColumn('Idade_3', F.expr('cast(Idade * 5 as string) as Idade_3'))\n",
    "\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfc1d7d",
   "metadata": {},
   "source": [
    "## Cardinalidade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4765949",
   "metadata": {},
   "source": [
    "- Aula teórica explicativa sobre cardinalidade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e56f099",
   "metadata": {},
   "source": [
    "### Dataframe de exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4f55a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cd_identificacao: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n",
      "+----------------+------------+\n",
      "|cd_identificacao|      status|\n",
      "+----------------+------------+\n",
      "|               1|        PAGO|\n",
      "|               2|    APROVADO|\n",
      "|               3|    RECUSADO|\n",
      "|               4|    ENTREGUE|\n",
      "|               5|   CANCELADO|\n",
      "|               6|NÃO ENTREGUE|\n",
      "+----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType\n",
    "\n",
    "spark = (\n",
    "    SparkSession.\n",
    "        builder.\n",
    "        getOrCreate()\n",
    ")\n",
    "# CLOUD --> spark.sql('select * from bd.aulas.tabelas')\n",
    "# df <- na leitura dos arquivos (.csv ou xlsx)\n",
    "## spark.read\n",
    "\n",
    "data = [\n",
    "    (\"1\", \"PAGO\"),\n",
    "    (\"2\", \"APROVADO\"),\n",
    "    (\"3\", \"RECUSADO\"),\n",
    "    (\"4\", \"ENTREGUE\"),\n",
    "    (\"5\", \"CANCELADO\"),\n",
    "    (\"6\", \"NÃO ENTREGUE\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"cd_identificacao\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac45cec",
   "metadata": {},
   "source": [
    "# Lendo arquivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907ba94c",
   "metadata": {},
   "source": [
    "## Command Separated Value (.csv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8c81002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------------+-------------+-----------+\n",
      "|cd_pedido|cd_cliente|             produto|status pedido|      valor|\n",
      "+---------+----------+--------------------+-------------+-----------+\n",
      "|       10|         2|TV LED 55''- SANSUNG|            2|R$ 5.500,48|\n",
      "|       12|         3|MÁQUINA DE LAVAR ...|            4|R$ 4.850,00|\n",
      "|       13|         5|NOTEBOOK LENOVO - i5|            3|R$ 3.585,50|\n",
      "|       15|         7|CELULAR POCO M4 -...|            1|R$ 1.285,00|\n",
      "|       12|         2|MÁQUINA DE LAVAR ...|            4|R$ 4.850,00|\n",
      "+---------+----------+--------------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "caminho = './dados/csv/'\n",
    "cliente = 'clientes.csv'\n",
    "status = 'status.csv'\n",
    "pedidos = 'pedidos.csv'\n",
    "\n",
    "clientes = (\n",
    "    spark.read.csv(\n",
    "        f'{caminho}{cliente}', \n",
    "        sep=';',\n",
    "        header=True\n",
    "    )\n",
    ")\n",
    "\n",
    "status = (\n",
    "    spark.read.csv(\n",
    "        f'{caminho}{status}', \n",
    "        sep=';',\n",
    "        header=True\n",
    "    )\n",
    ")\n",
    "\n",
    "pedidos = (\n",
    "    spark.read.csv(\n",
    "        f'{caminho}{pedidos}', \n",
    "        sep=';',\n",
    "        header=True\n",
    "    )\n",
    ")\n",
    "\n",
    "pedidos.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376f2ad7",
   "metadata": {},
   "source": [
    "## Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a2aad6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.classic.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "caminho = './dados/xlsx/'\n",
    "cliente = 'clientes.xlsx'\n",
    "aba = 'clientes'\n",
    "\n",
    "def ler_excel(file_path, aba):\n",
    "    try:\n",
    "        df = pd.read_excel(file_path, sheet_name=aba, engine='openpyxl')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Arquivo {file_path} não encontrado.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Erro ao ler a aba de nome {aba}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro inesperado: {e}\")\n",
    "    return df\n",
    "\n",
    "df = ler_excel(f'{caminho}{cliente}', aba)\n",
    "df = spark.createDataFrame(df)\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f649eb2a",
   "metadata": {},
   "source": [
    "# Intermediário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c28d6df",
   "metadata": {},
   "source": [
    "## Filtros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ae534",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d456be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5da915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----+---------------+\n",
      "|cd_cliente|        nome_cliente|sexo|data_nascimento|\n",
      "+----------+--------------------+----+---------------+\n",
      "|         1|      MARIA DO CARMO|   F|     1963-02-10|\n",
      "|         2|      LUCAS DA SILVA|   M|     1998-07-25|\n",
      "|         3| SILVÉRIO DA FONSECA|   M|     1975-01-15|\n",
      "|         4|    BRUNNO FERNANDES|   M|     2004-04-05|\n",
      "|         5|FERNANDA DO NASCI...|   F|     1994-09-12|\n",
      "|         6|  CARLOS DE OLIVEIRA|   M|     1998-04-15|\n",
      "|         7|  VITÓRIA DE ALMEIDA|   F|     1994-06-12|\n",
      "|         8|   GABRIELA DE SILVA|   F|     1994-03-27|\n",
      "|         9|      MARCOS PACHECO|   M|     1989-02-02|\n",
      "|        10|   DANIEL WANDERGAST|   M|     1980-01-24|\n",
      "+----------+--------------------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "caminho = './dados/csv/'\n",
    "cliente = 'clientes.csv'\n",
    "\n",
    "clientes = (\n",
    "    spark.read.csv(\n",
    "        f'{caminho}{cliente}', \n",
    "        sep=';',\n",
    "        header=True\n",
    "    )\n",
    ")\n",
    "\n",
    "clientes.show()\n",
    "clientes.createOrReplaceTempView(\"clientes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f67133f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----+---------------+\n",
      "|cd_cliente|        nome_cliente|sexo|data_nascimento|\n",
      "+----------+--------------------+----+---------------+\n",
      "|         1|      MARIA DO CARMO|   F|     1963-02-10|\n",
      "|         5|FERNANDA DO NASCI...|   F|     1994-09-12|\n",
      "|         7|  VITÓRIA DE ALMEIDA|   F|     1994-06-12|\n",
      "|         8|   GABRIELA DE SILVA|   F|     1994-03-27|\n",
      "+----------+--------------------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    select \n",
    "        * \n",
    "    from clientes\n",
    "    where sexo = 'F'\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e003331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----+---------------+\n",
      "|cd_cliente|        nome_cliente|sexo|data_nascimento|\n",
      "+----------+--------------------+----+---------------+\n",
      "|         1|      MARIA DO CARMO|   F|     1963-02-10|\n",
      "|         5|FERNANDA DO NASCI...|   F|     1994-09-12|\n",
      "|         7|  VITÓRIA DE ALMEIDA|   F|     1994-06-12|\n",
      "|         8|   GABRIELA DE SILVA|   F|     1994-03-27|\n",
      "+----------+--------------------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clientes2 = (\n",
    "    # para filtrar dados, o filter é a função utilizada\n",
    "    clientes\n",
    "        .filter(\n",
    "            F.col('sexo') == 'F'\n",
    "        )\n",
    ")\n",
    "clientes2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a548efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+----+---------------+\n",
      "|cd_cliente|       nome_cliente|sexo|data_nascimento|\n",
      "+----------+-------------------+----+---------------+\n",
      "|         2|     LUCAS DA SILVA|   M|     1998-07-25|\n",
      "|         3|SILVÉRIO DA FONSECA|   M|     1975-01-15|\n",
      "|         4|   BRUNNO FERNANDES|   M|     2004-04-05|\n",
      "|         6| CARLOS DE OLIVEIRA|   M|     1998-04-15|\n",
      "|         9|     MARCOS PACHECO|   M|     1989-02-02|\n",
      "|        10|  DANIEL WANDERGAST|   M|     1980-01-24|\n",
      "+----------+-------------------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clientes3 = (\n",
    "    # para filtrar dados, o filter é a função utilizada\n",
    "    clientes\n",
    "        .filter(\n",
    "            F.col('sexo') == 'M'\n",
    "        )\n",
    ")\n",
    "clientes3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b0673f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+----+---------------+\n",
      "|cd_cliente|       nome_cliente|sexo|data_nascimento|\n",
      "+----------+-------------------+----+---------------+\n",
      "|         2|     LUCAS DA SILVA|   M|     1998-07-25|\n",
      "|         3|SILVÉRIO DA FONSECA|   M|     1975-01-15|\n",
      "|         4|   BRUNNO FERNANDES|   M|     2004-04-05|\n",
      "|         6| CARLOS DE OLIVEIRA|   M|     1998-04-15|\n",
      "|         9|     MARCOS PACHECO|   M|     1989-02-02|\n",
      "|        10|  DANIEL WANDERGAST|   M|     1980-01-24|\n",
      "+----------+-------------------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clientes4 = (\n",
    "    # para filtrar dados, o filter é a função utilizada\n",
    "    clientes\n",
    "        .filter(\n",
    "            ~ (F.col('sexo') == 'F')\n",
    "        )\n",
    ")\n",
    "clientes4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce813ed1",
   "metadata": {},
   "source": [
    "### Isin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ef2c88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+----+---------------+\n",
      "|cd_cliente|       nome_cliente|sexo|data_nascimento|\n",
      "+----------+-------------------+----+---------------+\n",
      "|         2|     LUCAS DA SILVA|   M|     1998-07-25|\n",
      "|         3|SILVÉRIO DA FONSECA|   M|     1975-01-15|\n",
      "+----------+-------------------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    select\n",
    "        *\n",
    "    from clientes\n",
    "    where cd_cliente in ('2', '3')\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "474f68d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----+---------------+\n",
      "|cd_cliente|        nome_cliente|sexo|data_nascimento|\n",
      "+----------+--------------------+----+---------------+\n",
      "|         1|      MARIA DO CARMO|   F|     1963-02-10|\n",
      "|         4|    BRUNNO FERNANDES|   M|     2004-04-05|\n",
      "|         5|FERNANDA DO NASCI...|   F|     1994-09-12|\n",
      "|         6|  CARLOS DE OLIVEIRA|   M|     1998-04-15|\n",
      "|         7|  VITÓRIA DE ALMEIDA|   F|     1994-06-12|\n",
      "|         8|   GABRIELA DE SILVA|   F|     1994-03-27|\n",
      "|         9|      MARCOS PACHECO|   M|     1989-02-02|\n",
      "|        10|   DANIEL WANDERGAST|   M|     1980-01-24|\n",
      "+----------+--------------------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    select\n",
    "        *\n",
    "    from clientes\n",
    "    where cd_cliente not in ('2', '3')\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab9bfae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+----+---------------+\n",
      "|cd_cliente|       nome_cliente|sexo|data_nascimento|\n",
      "+----------+-------------------+----+---------------+\n",
      "|         2|     LUCAS DA SILVA|   M|     1998-07-25|\n",
      "|         3|SILVÉRIO DA FONSECA|   M|     1975-01-15|\n",
      "+----------+-------------------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clientes.filter(\n",
    "    F.col('cd_cliente').isin(['2', '3'])\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "029334e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----+---------------+\n",
      "|cd_cliente|        nome_cliente|sexo|data_nascimento|\n",
      "+----------+--------------------+----+---------------+\n",
      "|         1|      MARIA DO CARMO|   F|     1963-02-10|\n",
      "|         4|    BRUNNO FERNANDES|   M|     2004-04-05|\n",
      "|         5|FERNANDA DO NASCI...|   F|     1994-09-12|\n",
      "|         6|  CARLOS DE OLIVEIRA|   M|     1998-04-15|\n",
      "|         7|  VITÓRIA DE ALMEIDA|   F|     1994-06-12|\n",
      "|         8|   GABRIELA DE SILVA|   F|     1994-03-27|\n",
      "|         9|      MARCOS PACHECO|   M|     1989-02-02|\n",
      "|        10|   DANIEL WANDERGAST|   M|     1980-01-24|\n",
      "+----------+--------------------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clientes.filter(\n",
    "    ~(F.col('cd_cliente').isin(['2', '3']))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5869cc71",
   "metadata": {},
   "source": [
    "## Tratamentos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d834083d",
   "metadata": {},
   "source": [
    "## Strings para Números"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b737e3",
   "metadata": {},
   "source": [
    "### regex com Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3016a5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------------+-------------+-----------+\n",
      "|cd_pedido|cd_cliente|             produto|status pedido|      valor|\n",
      "+---------+----------+--------------------+-------------+-----------+\n",
      "|       10|         2|TV LED 55''- SANSUNG|            2|R$ 5.500,48|\n",
      "|       12|         3|MÁQUINA DE LAVAR ...|            4|R$ 4.850,00|\n",
      "|       13|         5|NOTEBOOK LENOVO - i5|            3|R$ 3.585,50|\n",
      "|       15|         7|CELULAR POCO M4 -...|            1|R$ 1.285,00|\n",
      "|       12|         2|MÁQUINA DE LAVAR ...|            4|R$ 4.850,00|\n",
      "+---------+----------+--------------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "caminho = './dados/csv/'\n",
    "pedidos = 'pedidos.csv'\n",
    "\n",
    "pedidos = (\n",
    "    spark.read.csv(\n",
    "        f'{caminho}{pedidos}', \n",
    "        sep=';',\n",
    "        header=True\n",
    "    )\n",
    ")\n",
    "\n",
    "pedidos.show()\n",
    "pedidos.createOrReplaceTempView(\"pedidos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76bd186b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cd_pedido: string (nullable = true)\n",
      " |-- cd_cliente: string (nullable = true)\n",
      " |-- produto: string (nullable = true)\n",
      " |-- status pedido: string (nullable = true)\n",
      " |-- valor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pedidos.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d93904e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------------+-------------+-----------+-----------+\n",
      "|cd_pedido|cd_cliente|             produto|status pedido|      valor|valor_limpo|\n",
      "+---------+----------+--------------------+-------------+-----------+-----------+\n",
      "|       10|         2|TV LED 55''- SANSUNG|            2|R$ 5.500,48|    5500.48|\n",
      "|       12|         3|MÁQUINA DE LAVAR ...|            4|R$ 4.850,00|     4850.0|\n",
      "|       13|         5|NOTEBOOK LENOVO - i5|            3|R$ 3.585,50|     3585.5|\n",
      "|       15|         7|CELULAR POCO M4 -...|            1|R$ 1.285,00|     1285.0|\n",
      "|       12|         2|MÁQUINA DE LAVAR ...|            4|R$ 4.850,00|     4850.0|\n",
      "+---------+----------+--------------------+-------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType, DoubleType\n",
    "\n",
    "pedidos2 = (\n",
    "    # MacOS é diferente do Windows\n",
    "    pedidos.withColumn('valor_limpo', F.regexp_replace(F.col('valor'),  r'R\\$\\s*', ''))\n",
    "           .withColumn('valor_limpo', F.regexp_replace(F.col('valor_limpo'), r'\\.', ''))\n",
    "           .withColumn('valor_limpo', F.regexp_replace(F.col('valor_limpo'), r',', '.'))\n",
    "           .withColumn('valor_limpo', F.col('valor_limpo').cast(DoubleType()))\n",
    ")\n",
    "\n",
    "pedidos2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce78d4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cd_pedido: string (nullable = true)\n",
      " |-- cd_cliente: string (nullable = true)\n",
      " |-- produto: string (nullable = true)\n",
      " |-- status pedido: string (nullable = true)\n",
      " |-- valor: string (nullable = true)\n",
      " |-- valor_limpo: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pedidos2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a46368a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+---------------+---------------+---------------+---------------+------------------+----------------+\n",
      "|cd_cliente|valor_mean_limpo|valor_sum_limpo|valor_max_limpo|valor_min_limpo|valor_avg_limpo|valor_median_limpo|valor_mode_limpo|\n",
      "+----------+----------------+---------------+---------------+---------------+---------------+------------------+----------------+\n",
      "|         7|          1285.0|         1285.0|         1285.0|         1285.0|         1285.0|            1285.0|          1285.0|\n",
      "|         3|          4850.0|         4850.0|         4850.0|         4850.0|         4850.0|            4850.0|          4850.0|\n",
      "|         5|          3585.5|         3585.5|         3585.5|         3585.5|         3585.5|            3585.5|          3585.5|\n",
      "|         2|         5175.24|       10350.48|        5500.48|         4850.0|        5175.24|           5175.24|         5500.48|\n",
      "+----------+----------------+---------------+---------------+---------------+---------------+------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pedidos2.groupBy('cd_cliente').agg(\n",
    "    F.mean('valor_limpo').alias('valor_mean_limpo'),\n",
    "    F.sum('valor_limpo').alias('valor_sum_limpo'),\n",
    "    F.max('valor_limpo').alias('valor_max_limpo'),\n",
    "    F.min('valor_limpo').alias('valor_min_limpo'),\n",
    "    F.avg('valor_limpo').alias('valor_avg_limpo'),\n",
    "    F.median('valor_limpo').alias('valor_median_limpo'),\n",
    "    F.mode('valor_limpo').alias('valor_mode_limpo')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ab098e",
   "metadata": {},
   "source": [
    "## Trabalhando com Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb3f8059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----+---------------+\n",
      "|cd_cliente|        nome_cliente|sexo|data_nascimento|\n",
      "+----------+--------------------+----+---------------+\n",
      "|         1|      MARIA DO CARMO|   F|     1963-02-10|\n",
      "|         2|      LUCAS DA SILVA|   M|     1998-07-25|\n",
      "|         3| SILVÉRIO DA FONSECA|   M|     1975-01-15|\n",
      "|         4|    BRUNNO FERNANDES|   M|     2004-04-05|\n",
      "|         5|FERNANDA DO NASCI...|   F|     1994-09-12|\n",
      "|         6|  CARLOS DE OLIVEIRA|   M|     1998-04-15|\n",
      "|         7|  VITÓRIA DE ALMEIDA|   F|     1994-06-12|\n",
      "|         8|   GABRIELA DE SILVA|   F|     1994-03-27|\n",
      "|         9|      MARCOS PACHECO|   M|     1989-02-02|\n",
      "|        10|   DANIEL WANDERGAST|   M|     1980-01-24|\n",
      "+----------+--------------------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "caminho = './dados/csv/'\n",
    "clientes = 'clientes.csv'\n",
    "\n",
    "clientes = (\n",
    "    spark.read.csv(\n",
    "        f'{caminho}{clientes}', \n",
    "        sep=';',\n",
    "        header=True\n",
    "    )\n",
    ")\n",
    "\n",
    "clientes.show()\n",
    "clientes.createOrReplaceTempView(\"clientes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7887ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes3 = (\n",
    "    clientes\n",
    "        .select(\"cd_cliente\", \"data_nascimento\")\n",
    "        .withColumn(\"data_nascimento2\", F.to_date(\n",
    "            F.col(\"data_nascimento\"), \"yyyy-MM-dd\"\n",
    "        ))\n",
    "        .withColumn(\"ano\", F.year(F.col(\"data_nascimento2\")))\n",
    "        .withColumn(\"mes\", F.month(F.col(\"data_nascimento2\")))\n",
    "        .withColumn(\"dia\", F.day(F.col(\"data_nascimento2\")))\n",
    "        \n",
    "        .withColumn(\n",
    "            \"data_BR\", \n",
    "                F.concat(\n",
    "                    F.col(\"dia\"), # dia\n",
    "                        F.lit(\"/\"), \n",
    "                    F.col(\"mes\"), # mes\n",
    "                        F.lit(\"/\"), \n",
    "                    F.col(\"ano\") # ano\n",
    "                )\n",
    "        )\n",
    "        .withColumn(\"data_BR\", F.to_date(\n",
    "            F.col(\"data_nascimento\"), \"dd/MM/yyyy\"\n",
    "        ))\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe27dfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+----------------+----+---+---+-----------+----------+\n",
      "|cd_cliente|data_nascimento|data_nascimento2| ano|mes|dia|data_BR_str|   data_BR|\n",
      "+----------+---------------+----------------+----+---+---+-----------+----------+\n",
      "|         1|     1963-02-10|      1963-02-10|1963|  2| 10| 10/02/1963|1963-02-10|\n",
      "|         2|     1998-07-25|      1998-07-25|1998|  7| 25| 25/07/1998|1998-07-25|\n",
      "|         3|     1975-01-15|      1975-01-15|1975|  1| 15| 15/01/1975|1975-01-15|\n",
      "|         4|     2004-04-05|      2004-04-05|2004|  4|  5| 05/04/2004|2004-04-05|\n",
      "|         5|     1994-09-12|      1994-09-12|1994|  9| 12| 12/09/1994|1994-09-12|\n",
      "|         6|     1998-04-15|      1998-04-15|1998|  4| 15| 15/04/1998|1998-04-15|\n",
      "|         7|     1994-06-12|      1994-06-12|1994|  6| 12| 12/06/1994|1994-06-12|\n",
      "|         8|     1994-03-27|      1994-03-27|1994|  3| 27| 27/03/1994|1994-03-27|\n",
      "|         9|     1989-02-02|      1989-02-02|1989|  2|  2| 02/02/1989|1989-02-02|\n",
      "|        10|     1980-01-24|      1980-01-24|1980|  1| 24| 24/01/1980|1980-01-24|\n",
      "+----------+---------------+----------------+----+---+---+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pode substituir a forma pela qual a conversão de data ocorre.\n",
    "        .withColumn(\n",
    "            \"data_BR_str\",\n",
    "            F.concat(\n",
    "                F.lpad(F.col(\"dia\"), 2, \"0\"), F.lit(\"/\"),\n",
    "                F.lpad(F.col(\"mes\"), 2, \"0\"), F.lit(\"/\"),\n",
    "                F.col(\"ano\")\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\"data_BR\", F.to_date(F.col(\"data_BR_str\"), \"dd/MM/yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4dadef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
