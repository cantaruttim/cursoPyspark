{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffb8edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.\n",
    "        builder.\n",
    "        appName(\"curso_pyspark\").\n",
    "        getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15364912",
   "metadata": {},
   "source": [
    "# Básico de Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32c03ee",
   "metadata": {},
   "source": [
    "## Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb6e648",
   "metadata": {},
   "source": [
    "<!-- ###\n",
    "### create table (\n",
    "###    nome VARCHAR(100) not null,\n",
    "###    sobrenome VARCHAR(100) not null,\n",
    "###    idade INT not null\n",
    "###)\n",
    "\n",
    "#  nome sobrenome idade\n",
    "# (N1      N2       I1) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a09180b",
   "metadata": {},
   "source": [
    "create table ( <br />\n",
    "   nome VARCHAR(100) not null, <br />\n",
    "   sobrenome VARCHAR(100) not null, <br />\n",
    "   idade INT not null <br />\n",
    ") <br />\n",
    "<br /><br />\n",
    "nome sobrenome idade\n",
    "(N1      N2       I1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9041fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n",
    "\n",
    "data = [\n",
    "    ##  C1          C2        C3\n",
    "    (\"Matheus\", \"Cantarutti\", 31),\n",
    "    (\"Ana\", \"Cláudia\", 18),\n",
    "    (\"Brunno\", \"Oliveira\", 25)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Nome\", StringType(), True),\n",
    "    StructField(\"Sobre_Nome\", StringType(), True),\n",
    "    StructField(\"Idade\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.printSchema()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f854be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"pessoas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b349c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SQL --> Pyspark\n",
    "\n",
    "spark.sql( \n",
    "'''    \n",
    "    select\n",
    "        *\n",
    "    from pessoas\n",
    "    where Idade < 20\n",
    "'''\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe949c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# df.filter('Idade < 20').show()\n",
    "df.filter(\n",
    "    F.col('Idade') < 20\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd31a82",
   "metadata": {},
   "source": [
    "## Tipo de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6e024d",
   "metadata": {},
   "source": [
    "- TIPO TEXTO/STRING >> abrangendo apenas as funções que tratam texto\n",
    "- TIPO DATA (DATA ESTÁ COM O TIPO DE STRING) >> Converter o seu texto para Data\n",
    "\n",
    "- FLOAT/DECIMAL e INTERGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "    select\n",
    "        *,\n",
    "        cast(Idade * 5 as string) as Idade_2\n",
    "    from pessoas\n",
    "''').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2451fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(   \n",
    "    # nome da coluna, expressões/funcao\n",
    "    df.withColumn('Idade_2', F.col('Idade') * 5)\n",
    "      .withColumn('data', F.lit('2025-01-01')) # current date\n",
    "      .withColumn('data2', F.to_date(F.col('data'), 'yyyy-MM-dd'))\n",
    "      .withColumn('Idade_3', F.expr('cast(Idade * 5 as string) as Idade_3'))\n",
    "\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfc1d7d",
   "metadata": {},
   "source": [
    "## Cardinalidade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4765949",
   "metadata": {},
   "source": [
    "- Aula teórica explicativa sobre cardinalidade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e56f099",
   "metadata": {},
   "source": [
    "## Dataframe de exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f55a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType\n",
    "\n",
    "spark = (\n",
    "    SparkSession.\n",
    "        builder.\n",
    "        getOrCreate()\n",
    ")\n",
    "# CLOUD --> spark.sql('select * from bd.aulas.tabelas')\n",
    "# df <- na leitura dos arquivos (.csv ou xlsx)\n",
    "## spark.read\n",
    "\n",
    "data = [\n",
    "    (\"1\", \"PAGO\"),\n",
    "    (\"2\", \"APROVADO\"),\n",
    "    (\"3\", \"RECUSADO\"),\n",
    "    (\"4\", \"ENTREGUE\"),\n",
    "    (\"5\", \"CANCELADO\"),\n",
    "    (\"6\", \"NÃO ENTREGUE\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"cd_identificacao\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac45cec",
   "metadata": {},
   "source": [
    "# Lendo arquivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907ba94c",
   "metadata": {},
   "source": [
    "## Command Separated Value (.csv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c81002",
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho = './dados/csv/'\n",
    "cliente = 'clientes.csv'\n",
    "status = 'status.csv'\n",
    "pedidos = 'pedidos.csv'\n",
    "\n",
    "clientes = (\n",
    "    spark.read.csv(\n",
    "        f'{caminho}{cliente}', \n",
    "        sep=';',\n",
    "        header=True\n",
    "    )\n",
    ")\n",
    "\n",
    "status = (\n",
    "    spark.read.csv(\n",
    "        f'{caminho}{status}', \n",
    "        sep=';',\n",
    "        header=True\n",
    "    )\n",
    ")\n",
    "\n",
    "pedidos = (\n",
    "    spark.read.csv(\n",
    "        f'{caminho}{pedidos}', \n",
    "        sep=';',\n",
    "        header=True\n",
    "    )\n",
    ")\n",
    "\n",
    "pedidos.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376f2ad7",
   "metadata": {},
   "source": [
    "## Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2aad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "caminho = './dados/xlsx/'\n",
    "cliente = 'clientes.xlsx'\n",
    "aba = 'clientes'\n",
    "\n",
    "def ler_excel(file_path, aba):\n",
    "    try:\n",
    "        df = pd.read_excel(file_path, sheet_name=aba, engine='openpyxl')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Arquivo {file_path} não encontrado.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Erro ao ler a aba de nome {aba}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro inesperado: {e}\")\n",
    "    return df\n",
    "\n",
    "df = ler_excel(f'{caminho}{cliente}', aba)\n",
    "df = spark.createDataFrame(df)\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f649eb2a",
   "metadata": {},
   "source": [
    "# Intermediário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c28d6df",
   "metadata": {},
   "source": [
    "## Filtros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ae534",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d456be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5da915",
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho = './dados/csv/'\n",
    "cliente = 'clientes.csv'\n",
    "\n",
    "clientes = (\n",
    "    spark.read.csv(\n",
    "        f'{caminho}{cliente}', \n",
    "        sep=';',\n",
    "        header=True\n",
    "    )\n",
    ")\n",
    "\n",
    "clientes.show()\n",
    "clientes.createOrReplaceTempView(\"clientes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67133f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "    select \n",
    "        * \n",
    "    from clientes\n",
    "    where sexo = 'F'\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e003331",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes2 = (\n",
    "    # para filtrar dados, o filter é a função utilizada\n",
    "    clientes\n",
    "        .filter(\n",
    "            F.col('sexo') == 'F'\n",
    "        )\n",
    ")\n",
    "clientes2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a548efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes3 = (\n",
    "    # para filtrar dados, o filter é a função utilizada\n",
    "    clientes\n",
    "        .filter(\n",
    "            F.col('sexo') == 'M'\n",
    "        )\n",
    ")\n",
    "clientes3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0673f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes4 = (\n",
    "    # para filtrar dados, o filter é a função utilizada\n",
    "    clientes\n",
    "        .filter(\n",
    "            ~ (F.col('sexo') == 'F')\n",
    "        )\n",
    ")\n",
    "clientes4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce813ed1",
   "metadata": {},
   "source": [
    "### Isin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef2c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "    select\n",
    "        *\n",
    "    from clientes\n",
    "    where cd_cliente in ('2', '3')\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474f68d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "    select\n",
    "        *\n",
    "    from clientes\n",
    "    where cd_cliente not in ('2', '3')\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9bfae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes.filter(\n",
    "    F.col('cd_cliente').isin(['2', '3'])\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029334e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes.filter(\n",
    "    ~(F.col('cd_cliente').isin(['2', '3']))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5869cc71",
   "metadata": {},
   "source": [
    "## Tratamentos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d834083d",
   "metadata": {},
   "source": [
    "### Strings para Números"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b737e3",
   "metadata": {},
   "source": [
    "### regex com Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho = './dados/csv/'\n",
    "pedidos = 'pedidos.csv'\n",
    "\n",
    "pedidos = (\n",
    "    spark.read.csv(\n",
    "        f'{caminho}{pedidos}', \n",
    "        sep=';',\n",
    "        header=True\n",
    "    )\n",
    ")\n",
    "\n",
    "pedidos.show()\n",
    "pedidos.createOrReplaceTempView(\"pedidos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bd186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pedidos.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93904e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType, DoubleType\n",
    "\n",
    "pedidos2 = (\n",
    "    # MacOS é diferente do Windows\n",
    "    pedidos.withColumn('valor_limpo', F.regexp_replace(F.col('valor'),  r'R\\$\\s*', ''))\n",
    "           .withColumn('valor_limpo', F.regexp_replace(F.col('valor_limpo'), r'\\.', ''))\n",
    "           .withColumn('valor_limpo', F.regexp_replace(F.col('valor_limpo'), r',', '.'))\n",
    "           .withColumn('valor_limpo', F.col('valor_limpo').cast(DoubleType()))\n",
    ")\n",
    "\n",
    "pedidos2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce78d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pedidos2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a46368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pedidos2.groupBy('cd_cliente').agg(\n",
    "    F.mean('valor_limpo').alias('valor_mean_limpo'),\n",
    "    F.sum('valor_limpo').alias('valor_sum_limpo'),\n",
    "    F.max('valor_limpo').alias('valor_max_limpo'),\n",
    "    F.min('valor_limpo').alias('valor_min_limpo'),\n",
    "    F.avg('valor_limpo').alias('valor_avg_limpo'),\n",
    "    F.median('valor_limpo').alias('valor_median_limpo'),\n",
    "    F.mode('valor_limpo').alias('valor_mode_limpo')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ab098e",
   "metadata": {},
   "source": [
    "### Trabalhando com Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e74e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.\n",
    "        builder.\n",
    "        appName(\"curso_pyspark\").\n",
    "        getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f8059",
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho = './dados/csv/'\n",
    "clientes = 'clientes.csv'\n",
    "\n",
    "clientes = (\n",
    "    spark.read.csv(\n",
    "        f'{caminho}{clientes}', \n",
    "        sep=';',\n",
    "        header=True\n",
    "    )\n",
    ")\n",
    "\n",
    "clientes.show()\n",
    "clientes.createOrReplaceTempView(\"clientes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7887ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes3 = (\n",
    "    clientes\n",
    "        .select(\"cd_cliente\", \"data_nascimento\")\n",
    "        .withColumn(\"data_nascimento2\", F.to_date(\n",
    "            F.col(\"data_nascimento\"), \"yyyy-MM-dd\"\n",
    "        ))\n",
    "        .withColumn(\"ano\", F.year(F.col(\"data_nascimento2\")))\n",
    "        .withColumn(\"mes\", F.month(F.col(\"data_nascimento2\")))\n",
    "        .withColumn(\"dia\", F.day(F.col(\"data_nascimento2\")))\n",
    "        \n",
    "        .withColumn(\n",
    "            \"data_BR\", \n",
    "                F.concat(\n",
    "                    F.col(\"dia\"), # dia\n",
    "                        F.lit(\"/\"), \n",
    "                    F.col(\"mes\"), # mes\n",
    "                        F.lit(\"/\"), \n",
    "                    F.col(\"ano\") # ano\n",
    "                )\n",
    "        )\n",
    "        .withColumn(\"data_BR\", F.try_to_timestamp(\n",
    "            F.col(\"data_BR\"), \"dd/MM/yyyy\"\n",
    "        ))\n",
    "\n",
    ")\n",
    "\n",
    "clientes3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe27dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pode substituir a forma pela qual a conversão de data ocorre.\n",
    "        .withColumn(\n",
    "            \"data_BR_str\",\n",
    "            F.concat(\n",
    "                F.lpad(F.col(\"dia\"), 2, \"0\"), F.lit(\"/\"),\n",
    "                F.lpad(F.col(\"mes\"), 2, \"0\"), F.lit(\"/\"),\n",
    "                F.col(\"ano\")\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\"data_BR\", F.to_date(F.col(\"data_BR_str\"), \"dd/MM/yyyy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3ce058",
   "metadata": {},
   "source": [
    "## Um pouco mais de Modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4dadef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType\n",
    "\n",
    "caminho = './dados/csv/'\n",
    "pedidos = 'pedidos.csv'\n",
    "clientes = 'clientes.csv'\n",
    "status = 'status.csv'\n",
    "\n",
    "pedidos = (\n",
    "    spark.read.csv(\n",
    "        f'{caminho}{pedidos}', \n",
    "        sep=';',\n",
    "        header=True\n",
    "    )\n",
    ")\n",
    "\n",
    "clientes = (\n",
    "    spark.read.csv(\n",
    "        f'{caminho}{clientes}', \n",
    "        sep=';',\n",
    "        header=True\n",
    "    )\n",
    ")\n",
    "\n",
    "status = (\n",
    "    spark.read.csv(\n",
    "        f'{caminho}{status}', \n",
    "        sep=';',\n",
    "        header=True\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a333e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pedidos.groupBy('cd_cliente').agg(\n",
    "    F.count('cd_cliente').alias('count')\n",
    ").orderBy(\n",
    "    F.col('count').desc()\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef9e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "pedidos.groupBy('cd_cliente').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49344e2c",
   "metadata": {},
   "source": [
    "# Avançado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ac8dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/10 22:00:21 WARN Utils: Your hostname, Matheuss-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.135 instead (on interface en0)\n",
      "25/08/10 22:00:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/10 22:00:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.135:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>curso_pyspark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x111c1a510>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType\n",
    "\n",
    "spark = (\n",
    "    SparkSession.\n",
    "        builder.\n",
    "        appName(\"curso_pyspark\").\n",
    "        getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2303c312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------------+-------------+-------+\n",
      "|cd_pedido|cd_cliente|             produto|status pedido| valor2|\n",
      "+---------+----------+--------------------+-------------+-------+\n",
      "|       10|         2|TV LED 55''- SANSUNG|            2|5500.48|\n",
      "|       12|         3|MÁQUINA DE LAVAR ...|            4| 4850.0|\n",
      "|       13|         5|NOTEBOOK LENOVO - i5|            3| 3585.5|\n",
      "|       15|         7|CELULAR POCO M4 -...|            1| 1285.0|\n",
      "|       12|         2|MÁQUINA DE LAVAR ...|            4| 4850.0|\n",
      "+---------+----------+--------------------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ler_arquivo(caminho, nome_arquivo):\n",
    "    arquivo = (\n",
    "        spark.read.csv(\n",
    "            f'{caminho}{nome_arquivo}',\n",
    "            sep=';',\n",
    "            header=True\n",
    "        )\n",
    "    )\n",
    "    return arquivo\n",
    "\n",
    "clientes = ler_arquivo('./dados/csv/', 'clientes.csv')\n",
    "status = ler_arquivo('./dados/csv/', 'status.csv')\n",
    "pedidos = ler_arquivo('./dados/csv/', 'pedidos.csv')\n",
    "\n",
    "\n",
    "pedidos = (    \n",
    "    \n",
    "    pedidos.withColumn('valor_limpo', F.regexp_replace(F.col('valor'),  r'R\\$\\s*', ''))\n",
    "           .withColumn('valor_limpo', F.regexp_replace(F.col('valor_limpo'), r'\\.', ''))\n",
    "           .withColumn('valor_limpo', F.regexp_replace(F.col('valor_limpo'), r',', '.'))\n",
    "           .withColumn('valor_limpo', F.col('valor_limpo').cast(DoubleType()))\n",
    "\n",
    "            # Renomear o nome de uma coluna\n",
    "           .withColumnRenamed(\"valor_limpo\", \"valor2\")\n",
    ").drop(\"valor\")\n",
    "\n",
    "pedidos.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08640b13",
   "metadata": {},
   "source": [
    "## Join - Movimentando informações entre tabelas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb6e69c",
   "metadata": {},
   "source": [
    "### Busca Horizontal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec16ea",
   "metadata": {},
   "source": [
    "#### Left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c45a05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---------+--------------------+-------+--------+--------------------+---------------+\n",
      "|cd_cliente|status_pedido|cd_pedido|             produto|  valor|  status|        nome_cliente|data_nascimento|\n",
      "+----------+-------------+---------+--------------------+-------+--------+--------------------+---------------+\n",
      "|         2|            2|       10|TV LED 55''- SANSUNG|5500.48|APROVADO|      LUCAS DA SILVA|     1998-07-25|\n",
      "|         3|            4|       12|MÁQUINA DE LAVAR ...| 4850.0|ENTREGUE| SILVÉRIO DA FONSECA|     1975-01-15|\n",
      "|         5|            3|       13|NOTEBOOK LENOVO - i5| 3585.5|RECUSADO|FERNANDA DO NASCI...|     1994-09-12|\n",
      "|         7|            1|       15|CELULAR POCO M4 -...| 1285.0|    PAGO|  VITÓRIA DE ALMEIDA|     1994-06-12|\n",
      "|         2|            4|       12|MÁQUINA DE LAVAR ...| 4850.0|ENTREGUE|      LUCAS DA SILVA|     1998-07-25|\n",
      "+----------+-------------+---------+--------------------+-------+--------+--------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "status = status.withColumnRenamed(\"cd_identificacao\", \"status_pedido\")\n",
    "clientes = clientes.select(\"cd_cliente\", \"nome_cliente\", \"data_nascimento\")\n",
    "\n",
    "pedidos = (\n",
    "    pedidos.withColumnRenamed(\"valor2\", \"valor\")\n",
    "           .withColumnRenamed(\"status pedido\", \"status_pedido\")\n",
    "            # renomeia a coluna na tabela de pedidos ou renomeamos a coluna na tabela de status\n",
    ")\n",
    "            # Tabela, coluna(s), como = (left, right, inner)\n",
    "pedidos = (\n",
    "    pedidos.join(status, on=\"status_pedido\", how=\"left\")\n",
    "           .join(clientes, on=\"cd_cliente\", how=\"left\")\n",
    ")\n",
    "\n",
    "pedidos.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5923dbe2",
   "metadata": {},
   "source": [
    "#### Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1305a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ler_arquivo(caminho, nome_arquivo):\n",
    "    arquivo = (\n",
    "        spark.read.csv(\n",
    "            f'{caminho}{nome_arquivo}',\n",
    "            sep=';',\n",
    "            header=True\n",
    "        )\n",
    "    )\n",
    "    return arquivo\n",
    "\n",
    "clientes = ler_arquivo('./dados/csv/', 'clientes.csv')\n",
    "pedidos = ler_arquivo('./dados/csv/', 'pedidos.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de3038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+-------------+-----+------------------+----+---------------+\n",
      "|cd_cliente|cd_pedido|produto|status pedido|valor|      nome_cliente|sexo|data_nascimento|\n",
      "+----------+---------+-------+-------------+-----+------------------+----+---------------+\n",
      "|         1|     NULL|   NULL|         NULL| NULL|    MARIA DO CARMO|   F|     1963-02-10|\n",
      "|         4|     NULL|   NULL|         NULL| NULL|  BRUNNO FERNANDES|   M|     2004-04-05|\n",
      "|         6|     NULL|   NULL|         NULL| NULL|CARLOS DE OLIVEIRA|   M|     1998-04-15|\n",
      "|         8|     NULL|   NULL|         NULL| NULL| GABRIELA DE SILVA|   F|     1994-03-27|\n",
      "|         9|     NULL|   NULL|         NULL| NULL|    MARCOS PACHECO|   M|     1989-02-02|\n",
      "|        10|     NULL|   NULL|         NULL| NULL| DANIEL WANDERGAST|   M|     1980-01-24|\n",
      "+----------+---------+-------+-------------+-----+------------------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    pedidos.join(clientes, on=\"cd_cliente\", how=\"right\")\n",
    "        .filter(\n",
    "            F.col(\"cd_pedido\").isNull()\n",
    "        )\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76a04c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----+---------------+---------+-------+-------------+-----+\n",
      "|cd_cliente|      nome_cliente|sexo|data_nascimento|cd_pedido|produto|status pedido|valor|\n",
      "+----------+------------------+----+---------------+---------+-------+-------------+-----+\n",
      "|         1|    MARIA DO CARMO|   F|     1963-02-10|     NULL|   NULL|         NULL| NULL|\n",
      "|         4|  BRUNNO FERNANDES|   M|     2004-04-05|     NULL|   NULL|         NULL| NULL|\n",
      "|         6|CARLOS DE OLIVEIRA|   M|     1998-04-15|     NULL|   NULL|         NULL| NULL|\n",
      "|         8| GABRIELA DE SILVA|   F|     1994-03-27|     NULL|   NULL|         NULL| NULL|\n",
      "|         9|    MARCOS PACHECO|   M|     1989-02-02|     NULL|   NULL|         NULL| NULL|\n",
      "|        10| DANIEL WANDERGAST|   M|     1980-01-24|     NULL|   NULL|         NULL| NULL|\n",
      "+----------+------------------+----+---------------+---------+-------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clientes.join(pedidos, on=\"cd_cliente\", how=\"left\").filter(F.col(\"cd_pedido\").isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a44fd94",
   "metadata": {},
   "source": [
    "#### Inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e18a6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+-------------+-----------+--------------------+----+---------------+\n",
      "|cd_cliente|cd_pedido|             produto|status pedido|      valor|        nome_cliente|sexo|data_nascimento|\n",
      "+----------+---------+--------------------+-------------+-----------+--------------------+----+---------------+\n",
      "|         2|       12|MÁQUINA DE LAVAR ...|            4|R$ 4.850,00|      LUCAS DA SILVA|   M|     1998-07-25|\n",
      "|         2|       10|TV LED 55''- SANSUNG|            2|R$ 5.500,48|      LUCAS DA SILVA|   M|     1998-07-25|\n",
      "|         3|       12|MÁQUINA DE LAVAR ...|            4|R$ 4.850,00| SILVÉRIO DA FONSECA|   M|     1975-01-15|\n",
      "|         5|       13|NOTEBOOK LENOVO - i5|            3|R$ 3.585,50|FERNANDA DO NASCI...|   F|     1994-09-12|\n",
      "|         7|       15|CELULAR POCO M4 -...|            1|R$ 1.285,00|  VITÓRIA DE ALMEIDA|   F|     1994-06-12|\n",
      "+----------+---------+--------------------+-------------+-----------+--------------------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pedidos.join(clientes, on=\"cd_cliente\", how=\"inner\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b005534f",
   "metadata": {},
   "source": [
    "### 'Busca' vertical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd3a11ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ler_arquivo(caminho, nome_arquivo):\n",
    "    arquivo = (\n",
    "        spark.read.csv(\n",
    "            f'{caminho}{nome_arquivo}',\n",
    "            sep=';',\n",
    "            header=True\n",
    "        )\n",
    "    )\n",
    "    return arquivo\n",
    "\n",
    "status = ler_arquivo('./dados/csv/', 'status.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d4f9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eaa960ee",
   "metadata": {},
   "source": [
    "## Condicionais com When"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab07595c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de8e46cb",
   "metadata": {},
   "source": [
    "## Pivot tables\n",
    "### Transformando visões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01461b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
